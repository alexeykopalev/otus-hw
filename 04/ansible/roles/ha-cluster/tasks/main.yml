# - name: Get list of active repositories
#   command: yum repolist
#   args:
#     warn: "{{ (ansible_version.full is version('2.14', '<')) | ternary(false, omit) }}"
#   register: yum_repolist
#   changed_when: false
#   check_mode: false

# - name: Put SELinux in permissive mode, logging actions that would be blocked.
#   ansible.posix.selinux:
#     policy: targeted
#     state: permissive

- name: Enable highavailability repository (AlmaLinux)
  ini_file:
    dest: '/etc/yum.repos.d/almalinux-ha.repo'
    section: 'ha'
    option: 'enabled'
    value: '1'
    create: 'no'
    mode: '0644'

- name: Enable resilientstorage repository (AlmaLinux)
  ini_file:
    dest: '/etc/yum.repos.d/almalinux-resilientstorage.repo'
    section: 'resilientstorage'
    option: 'enabled'
    value: '1'
    create: 'no'
    mode: '0644'

- name: Install the Packages
  yum:
    name:
      - pacemaker
      - pcs
      - fence-agents-all
      - lvm2
      - lvm2-lockd
      - dlm
      - gfs2-utils
    state: present

- name: Add 'use_lvmlockd = 1' to LVM Confifuration File
  ansible.builtin.lineinfile:
    path: /etc/lvm/lvm.conf
    regexp: "{{ item.regexp }}"
    line: "{{ item.line }}"
  with_items: 
    - { regexp: "system_id_source =", line: "system_id_source = 'uname'" }
    - { regexp: "use_lvmlockd =", line: "use_lvmlockd = 1" }
    - { regexp: "use_devicesfile =", line: "use_devicesfile = 1" }

# echo 'locking_type = 1' >> /etc/lvm/lvm.conf
- name: Add2 'locking_type = 1' to LVM Confifuration File
  ansible.builtin.lineinfile:
    path: /etc/lvm/lvm.conf
    insertafter: '^global {'
    line: locking_type = 1
  
- name: Start the lvmlockd service
  service:
    name: "lvmlockd"
    state: started
    enabled: true

- name: Create directories for cluster mount
  ansible.builtin.file:
    path: /var/www
    state: directory
    mode: '0755'

- name: Start the pcs service
  service:
    name: "pcsd"
    state: restarted
    enabled: true

- name: Set the password for the use hacluster
  user:
    name: "{{ username }}"
    password: "{{ password | password_hash('sha512')}}"
    update_password: always

- name: Gather cluster status
  command: "pcs cluster status"
  register: cluster_status
  ignore_errors: true

- name: Authenticate the cluster
  command: "pcs host auth {{ node1 }} {{ node2 }} -u {{ username }} -p {{ password }}"
  run_once: true
  when: cluster_status.rc | int != 0

- name: Setup the HA-Cluster
  command: "pcs cluster setup {{ cluster_name }} {{ node1 }} {{ node2 }}"
  run_once: true
  when: cluster_status.rc | int != 0

- name: Start the HA-Cluster
  command: "pcs cluster start --all"
  run_once: true
  when: cluster_status.rc | int != 0

- name: Enable the HA-Cluster
  command: "pcs cluster enable --all"
  run_once: true
  when: cluster_status.rc | int != 0

- name: "Setting stonith-enabled=false"
  command: "pcs property set stonith-enabled=false"
  run_once: true

- name: "Setting 'No Quorum' Policy"
  command: "pcs property set no-quorum-policy=freeze"
  run_once: true

- name: Gather dlm resource status
  command: "pcs resource show dlm-clone"
  register: resource_dlm_clone_status
  ignore_errors: true

- name: "Creating 'dlm' Resource"
  command: pcs resource create dlm systemd:dlm op monitor interval=30s on-fail=ignore clone interleave=true ordered=true
  run_once: true
  when: resource_dlm_clone_status.rc | int != 0

- name: Gather lvmlockd resource status
  command: "pcs resource show lvmlockd-clone"
  register: resource_lvmlockd_clone_status
  ignore_errors: true

- name: "Creating lvmlockd Resource"
  command: pcs resource create lvmlockd ocf:heartbeat:lvmlockd op monitor interval=10s on-fail=ignore clone interleave=true ordered=true
  run_once: true
  when: resource_lvmlockd_clone_status.rc | int != 0

- name: "Creating First Constraint"
  command: pcs constraint order start dlm-clone then lvmlockd-clone
  run_once: true
  when: resource_lvmlockd_clone_status.rc | int != 0 or resource_dlm_clone_status.rc | int != 0

- name: "Pause"
  pause:
    seconds: 10
  run_once: true

- name: "Gather pvs /dev/sda"
  shell: pvs | grep /dev/sda
  run_once: true
  register: pvs_status
  ignore_errors: true

- name: "Creating Physical Volume"
  command: pvcreate /dev/sda #mapper/otusdisk
  run_once: true
  when: pvs_status.rc | int != 0

- name: "Gather vgs /dev/sda"
  shell: vgs | grep cluster_vg
  run_once: true
  register: vgs_status
  ignore_errors: true

- name: "Creating Volume Group"
  command: vgcreate --shared cluster_vg /dev/sda #mapper/otusdisk
  run_once: true
  when: vgs_status.rc | int != 0

# vgchange --lockstart web_cluster_vg
- name: start lock manager for shared
  ansible.builtin.command: "vgchange --lockstart cluster_vg"
  when: "{{ item }}"
  with_items: 
    - inventory_hostname == play_hosts[1] 
    - vgs_status.rc | int != 0

- name: "Pause"
  pause:
    seconds: 10
  run_once: true

- name: "Gather lvs | grep cluster_lv"
  shell: lvs | grep cluster_lv
  run_once: true
  register: lvs_status
  ignore_errors: true

- name: "Creating Logical Volume"
  command: lvcreate --activate sy -L10G -n cluster_lv cluster_vg
  run_once: true
  when: lvs_status.rc | int != 0

- name: "Formatting Logical Volume With GFS2"
  command: mkfs.gfs2 -O -j3 -p lock_dlm -t otuscluster:gfs2 /dev/cluster_vg/cluster_lv
  run_once: true
  when: lvs_status.rc | int != 0

- name: Gather cluster_vg-clone resource status
  command: "pcs resource show cluster_vg-clone"
  register: resource_cluster_vg_clone_status
  ignore_errors: true

- name: create LVM-activate resource
  ansible.builtin.command: pcs resource create cluster_vg ocf:heartbeat:LVM-activate lvname=cluster_lv vgname=cluster_vg activation_mode=shared vg_access_mode=lvmlockd op monitor interval=30s on-fail=ignore clone interleave=true ordered=true
  run_once: true
  when: resource_cluster_vg_clone_status.rc | int != 0

# pcs constraint order start lvmlockd-clone then web_cluster_vg-clone
- name: set start order as [locking] → [shared_vg]
  ansible.builtin.command: pcs constraint order start lvmlockd-clone then cluster_vg-clone
  run_once: true
  when: resource_cluster_vg_clone_status.rc | int != 0

# pcs constraint colocation add web_cluster_vg-clone with lvmlockd-clone
- name: set that [shared_vg] and [locking] start on a same node
  ansible.builtin.command: pcs constraint colocation add cluster_vg-clone with lvmlockd-clone
  run_once: true
  when: resource_cluster_vg_clone_status.rc | int != 0

- name: Gather clusterfs-clone resource status
  command: "pcs resource show clusterfs-clone"
  register: resource_clusterfs_clone_status
  ignore_errors: true

- name: "Creating Cluster File System"
  command: pcs resource create clusterfs Filesystem \
   device="/dev/cluster_vg/cluster_lv" directory="/var/www" \
   fstype="gfs2" "options=noatime" op monitor interval=10s \
   on-fail=ignore clone interleave=true
  run_once: true
  when: resource_clusterfs_clone_status.rc | int != 0

# pcs constraint order start web_cluster_vg-clone then web_cluster_fs-clone
- name: set start order as [shared_vg] → [shared_fs]
  ansible.builtin.command: pcs constraint order start cluster_vg-clone then clusterfs-clone
  run_once: true
  when: resource_clusterfs_clone_status.rc | int != 0

# pcs constraint colocation add web_cluster_fs-clone with web_cluster_vg-clone
- name: set that [shared_fs] and [shared_vg] start on a same node
  ansible.builtin.command: pcs constraint colocation add clusterfs-clone with cluster_vg-clone
  run_once: true
  when: resource_clusterfs_clone_status.rc | int != 0

