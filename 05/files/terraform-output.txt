[alexey@fedora 05]$ terraform apply

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # yandex_alb_backend_group.alb-bg will be created
  + resource "yandex_alb_backend_group" "alb-bg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + name       = "alb-bg"

      + http_backend {
          + name             = "backend-1"
          + port             = 80
          + target_group_ids = (known after apply)
          + weight           = 1

          + healthcheck {
              + healthcheck_port = 80
              + interval         = "10s"
              + timeout          = "10s"

              + http_healthcheck {
                  + path = "/"
                }
            }
        }
    }

  # yandex_alb_http_router.alb-router will be created
  + resource "yandex_alb_http_router" "alb-router" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + name       = "alb-router"
    }

  # yandex_alb_load_balancer.alb-1 will be created
  + resource "yandex_alb_load_balancer" "alb-1" {
      + created_at         = (known after apply)
      + folder_id          = (known after apply)
      + id                 = (known after apply)
      + log_group_id       = (known after apply)
      + name               = "alb-1"
      + network_id         = (known after apply)
      + security_group_ids = (known after apply)
      + status             = (known after apply)

      + allocation_policy {
          + location {
              + disable_traffic = false
              + subnet_id       = (known after apply)
              + zone_id         = "ru-central1-a"
            }
          + location {
              + disable_traffic = false
              + subnet_id       = (known after apply)
              + zone_id         = "ru-central1-b"
            }
        }

      + listener {
          + name = "alb-listener"

          + endpoint {
              + ports = [
                  + 80,
                ]

              + address {
                  + external_ipv4_address {
                      + address = "158.160.75.138"
                    }
                }
            }

          + http {
              + handler {
                  + allow_http10       = false
                  + http_router_id     = (known after apply)
                  + rewrite_request_id = false
                }
            }
        }
    }

  # yandex_alb_target_group.alb-tg will be created
  + resource "yandex_alb_target_group" "alb-tg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + name       = "alb-tg"

      + target {
          + ip_address = "10.10.1.10"
          + subnet_id  = (known after apply)
        }
      + target {
          + ip_address = "10.10.2.10"
          + subnet_id  = (known after apply)
        }
    }

  # yandex_alb_virtual_host.alb-host will be created
  + resource "yandex_alb_virtual_host" "alb-host" {
      + authority      = [
          + "dip-akopalev.ru",
        ]
      + http_router_id = (known after apply)
      + id             = (known after apply)
      + name           = "alb-host"

      + route {
          + name = "route-1"

          + http_route {
              + http_route_action {
                  + backend_group_id = (known after apply)
                }
            }
        }
    }

  # yandex_compute_disk.iscsi-disk[0] will be created
  + resource "yandex_compute_disk" "iscsi-disk" {
      + block_size  = 4096
      + created_at  = (known after apply)
      + folder_id   = (known after apply)
      + id          = (known after apply)
      + name        = "iscsi-disk-01"
      + product_ids = (known after apply)
      + size        = 15
      + status      = (known after apply)
      + type        = "network-hdd"
      + zone        = "ru-central1-a"
    }

  # yandex_dns_recordset.rs-1 will be created
  + resource "yandex_dns_recordset" "rs-1" {
      + data    = [
          + "158.160.75.138",
        ]
      + id      = (known after apply)
      + name    = "dip-akopalev.ru."
      + ttl     = 600
      + type    = "A"
      + zone_id = (known after apply)
    }

  # yandex_dns_recordset.rs-2 will be created
  + resource "yandex_dns_recordset" "rs-2" {
      + data    = [
          + "dip-akopalev.ru",
        ]
      + id      = (known after apply)
      + name    = "www"
      + ttl     = 600
      + type    = "CNAME"
      + zone_id = (known after apply)
    }

  # yandex_dns_zone.zone1 will be created
  + resource "yandex_dns_zone" "zone1" {
      + created_at       = (known after apply)
      + description      = "Public zone"
      + folder_id        = (known after apply)
      + id               = (known after apply)
      + name             = "zone1"
      + private_networks = (known after apply)
      + public           = true
      + zone             = "dip-akopalev.ru."
    }

  # module.backends["backend1"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "backend1"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "backend1"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.1.10"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.backends["backend2"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "backend2"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "backend2"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-b"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.2.10"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.backends["backend3"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "backend3"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "backend3"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-c"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.3.10"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.bast-host.yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "bast-host-srv"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "bast-host-srv"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-b"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd87t5gt48uc6feiibm8"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "172.16.16.254"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = true
          + nat_ip_address     = "158.160.84.164"
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = false
        }
    }

  # module.db-servers["db-srv1"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "db-srv1"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "db-srv1"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.1.4"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.db-servers["db-srv2"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "db-srv2"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "db-srv2"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-b"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.2.4"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.db-servers["db-srv3"].yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "db-srv3"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "db-srv3"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-c"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.3.4"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = true
        }
    }

  # module.iscsi-srv.yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "iscsi-srv"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "iscsi-srv"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.1.3"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = false
        }

      + secondary_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = "READ_WRITE"
        }
    }

  # module.network-create.yandex_vpc_gateway.nat_gateway will be created
  + resource "yandex_vpc_gateway" "nat_gateway" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "test-gateway"

      + shared_egress_gateway {}
    }

  # module.network-create.yandex_vpc_network.network-1 will be created
  + resource "yandex_vpc_network" "network-1" {
      + created_at                = (known after apply)
      + default_security_group_id = (known after apply)
      + folder_id                 = (known after apply)
      + id                        = (known after apply)
      + labels                    = (known after apply)
      + name                      = "network1"
      + subnet_ids                = (known after apply)
    }

  # module.network-create.yandex_vpc_route_table.rt will be created
  + resource "yandex_vpc_route_table" "rt" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "test-route-table"
      + network_id = (known after apply)

      + static_route {
          + destination_prefix = "0.0.0.0/0"
          + gateway_id         = (known after apply)
        }
    }

  # module.network-create.yandex_vpc_subnet.subnet-1 will be created
  + resource "yandex_vpc_subnet" "subnet-1" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "subnet-1"
      + network_id     = (known after apply)
      + route_table_id = (known after apply)
      + v4_cidr_blocks = [
          + "10.10.1.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-a"
    }

  # module.network-create.yandex_vpc_subnet.subnet-2 will be created
  + resource "yandex_vpc_subnet" "subnet-2" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "subnet-2"
      + network_id     = (known after apply)
      + route_table_id = (known after apply)
      + v4_cidr_blocks = [
          + "10.10.2.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-b"
    }

  # module.network-create.yandex_vpc_subnet.subnet-3 will be created
  + resource "yandex_vpc_subnet" "subnet-3" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "subnet-3"
      + network_id     = (known after apply)
      + route_table_id = (known after apply)
      + v4_cidr_blocks = [
          + "10.10.3.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-c"
    }

  # module.network-create.yandex_vpc_subnet.subnet-bast will be created
  + resource "yandex_vpc_subnet" "subnet-bast" {
      + created_at     = (known after apply)
      + folder_id      = (known after apply)
      + id             = (known after apply)
      + labels         = (known after apply)
      + name           = "subnet-bast"
      + network_id     = (known after apply)
      + v4_cidr_blocks = [
          + "172.16.16.0/24",
        ]
      + v6_cidr_blocks = (known after apply)
      + zone           = "ru-central1-b"
    }

  # module.proxysql-srv.yandex_compute_instance.vm will be created
  + resource "yandex_compute_instance" "vm" {
      + allow_stopping_for_update = true
      + created_at                = (known after apply)
      + folder_id                 = (known after apply)
      + fqdn                      = (known after apply)
      + gpu_cluster_id            = (known after apply)
      + hostname                  = "proxysql-srv"
      + id                        = (known after apply)
      + metadata                  = {
          + "user-data" = <<-EOT
                #cloud-config
                users:
                  - name: akopalev
                    groups: wheel
                    shell: /bin/bash
                    sudo: ['ALL=(ALL) NOPASSWD:ALL']
                    ssh_authorized_keys:
                      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxHlMc4ySp0W7oIBf1aRAxdIaoFyx3IZ+PNpAvDgluLZxmNLJ22ImYIcQkloY9tLwhD6guIdkoWtSusrjPfAO5bEPwKfa5GI51Qoq76gZ5/KyMtnYAARDyuSbMjeqaAJaF71oGKC4032hTlXtvMf7wAy8nfrP3zrRE7PDsqLh5vuVctAa78SFHp92394GYU0LkeCbE8dN+RW7T1wFoK7jK2HfVfLZMXtiJT3pji7jtkB7SKW8hNCeojKylZSW/AQhEyo32aPjZHomtQDkJ4DPVkPiGmcpDtpQ5u0fm0soEkODlKzECiNZM2pQq/gRdSmvObMbCkot5yjS4+uAqxl4jnzJU57lWDtFrIvkBDGJ2Y564r/pctq9uWp/QSbPgEv8uT/QaIoVeYMO723HHSzLRPVslrQiwkCU1qTFFHFCA2OlGJLZF05mGEi7pDAv2EX6PmfTdKwh8IsamEUgZsYbo+/RWKi56VDFGsZttrW3kVQ9mYEUAgitkDWnwpFO8DVU= alexey@vivobook-fedora
            EOT
        }
      + name                      = "proxysql-srv"
      + network_acceleration_type = "standard"
      + platform_id               = "standard-v1"
      + service_account_id        = (known after apply)
      + status                    = (known after apply)
      + zone                      = "ru-central1-a"

      + boot_disk {
          + auto_delete = true
          + device_name = (known after apply)
          + disk_id     = (known after apply)
          + mode        = (known after apply)

          + initialize_params {
              + block_size  = (known after apply)
              + description = (known after apply)
              + image_id    = "fd81prb1447ilqb2mp3m"
              + name        = (known after apply)
              + size        = 10
              + snapshot_id = (known after apply)
              + type        = "network-hdd"
            }
        }

      + network_interface {
          + index              = (known after apply)
          + ip_address         = "10.10.1.5"
          + ipv4               = true
          + ipv6               = (known after apply)
          + ipv6_address       = (known after apply)
          + mac_address        = (known after apply)
          + nat                = false
          + nat_ip_address     = (known after apply)
          + nat_ip_version     = (known after apply)
          + security_group_ids = (known after apply)
          + subnet_id          = (known after apply)
        }

      + resources {
          + core_fraction = 20
          + cores         = 2
          + memory        = 2
        }

      + scheduling_policy {
          + preemptible = false
        }
    }

  # module.sg-create.yandex_vpc_security_group.db-sg will be created
  + resource "yandex_vpc_security_group" "db-sg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "db-sg"
      + network_id = (known after apply)
      + status     = (known after apply)

      + egress {
          + description    = "any"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ANY"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }

      + ingress {
          + description    = "allow 3306 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3306
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 3306 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3306
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 3306 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3306
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4444 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4444
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4444 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4444
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4444 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4444
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4567 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4567
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4567 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4567
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4567 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4567
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4568 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4568
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4568 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4568
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 4568 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 4568
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description       = "allow ssh bastion"
          + from_port         = -1
          + id                = (known after apply)
          + labels            = (known after apply)
          + port              = 22
          + protocol          = "TCP"
          + security_group_id = (known after apply)
          + to_port           = -1
          + v4_cidr_blocks    = []
          + v6_cidr_blocks    = []
        }
      + ingress {
          + description    = "ping allow"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ICMP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
    }

  # module.sg-create.yandex_vpc_security_group.external-sg will be created
  + resource "yandex_vpc_security_group" "external-sg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "external-sg"
      + network_id = (known after apply)
      + status     = (known after apply)

      + egress {
          + description    = "any"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ANY"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }

      + ingress {
          + description       = "allow ssh bastion"
          + from_port         = -1
          + id                = (known after apply)
          + labels            = (known after apply)
          + port              = 22
          + protocol          = "TCP"
          + security_group_id = (known after apply)
          + to_port           = -1
          + v4_cidr_blocks    = []
          + v6_cidr_blocks    = []
        }
      + ingress {
          + description    = "ext-http"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 80
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "ext-https"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 443
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "healthchecks"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 30080
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "198.18.235.0/24",
              + "198.18.248.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "ping allow"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ICMP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
    }

  # module.sg-create.yandex_vpc_security_group.internal-sg will be created
  + resource "yandex_vpc_security_group" "internal-sg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "internal-sg"
      + network_id = (known after apply)
      + status     = (known after apply)

      + egress {
          + description    = "any"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ANY"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }

      + ingress {
          + description    = "allow CLVM subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 21064
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow CLVM subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 21064
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow CLVM subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 21064
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow booth ticket manager subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 9929
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync multicast-udp subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5404
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync multicast-udp subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5404
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync multicast-udp subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5404
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5405
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5405
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5405
          + protocol       = "UDP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync-qnetd subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5403
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync-qnetd subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5403
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow corosync-qnetd subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 5403
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow crmd port subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3121
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow crmd port subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3121
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow crmd port subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3121
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow http subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 80
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow http subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 80
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow http subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 80
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow iscsi 3620 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3260
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow iscsi 3620 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3260
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow iscsi 3620 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 3260
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow pcsd port subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 2224
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow pcsd port subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 2224
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow pcsd port subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 2224
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description       = "allow ssh bastion"
          + from_port         = -1
          + id                = (known after apply)
          + labels            = (known after apply)
          + port              = 22
          + protocol          = "TCP"
          + security_group_id = (known after apply)
          + to_port           = -1
          + v4_cidr_blocks    = []
          + v6_cidr_blocks    = []
        }
      + ingress {
          + description       = "balancer"
          + from_port         = -1
          + id                = (known after apply)
          + labels            = (known after apply)
          + port              = 80
          + protocol          = "TCP"
          + security_group_id = (known after apply)
          + to_port           = -1
          + v4_cidr_blocks    = []
          + v6_cidr_blocks    = []
        }
      + ingress {
          + description    = "ping allow"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ICMP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
    }

  # module.sg-create.yandex_vpc_security_group.proxysql-sg will be created
  + resource "yandex_vpc_security_group" "proxysql-sg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "proxysql-sg"
      + network_id = (known after apply)
      + status     = (known after apply)

      + egress {
          + description    = "any"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ANY"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }

      + ingress {
          + description    = "allow 6032 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6032
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 6032 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6032
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 6032 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6032
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 6033 subnet-1"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6033
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.1.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 6033 subnet-2"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6033
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.2.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "allow 6033 subnet-3"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 6033
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "10.10.3.0/24",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description       = "allow ssh bastion"
          + from_port         = -1
          + id                = (known after apply)
          + labels            = (known after apply)
          + port              = 22
          + protocol          = "TCP"
          + security_group_id = (known after apply)
          + to_port           = -1
          + v4_cidr_blocks    = []
          + v6_cidr_blocks    = []
        }
      + ingress {
          + description    = "ping allow"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ICMP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
    }

  # module.sg-create.yandex_vpc_security_group.sec-bast-sg will be created
  + resource "yandex_vpc_security_group" "sec-bast-sg" {
      + created_at = (known after apply)
      + folder_id  = (known after apply)
      + id         = (known after apply)
      + labels     = (known after apply)
      + name       = "sec-bast-sg"
      + network_id = (known after apply)
      + status     = (known after apply)

      + egress {
          + description    = "any"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ANY"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }

      + ingress {
          + description    = "ping allow"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = -1
          + protocol       = "ICMP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
      + ingress {
          + description    = "ssh in"
          + from_port      = -1
          + id             = (known after apply)
          + labels         = (known after apply)
          + port           = 22
          + protocol       = "TCP"
          + to_port        = -1
          + v4_cidr_blocks = [
              + "0.0.0.0/0",
            ]
          + v6_cidr_blocks = []
        }
    }

Plan: 30 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

yandex_alb_http_router.alb-router: Creating...
module.network-create.yandex_vpc_network.network-1: Creating...
module.network-create.yandex_vpc_gateway.nat_gateway: Creating...
yandex_compute_disk.iscsi-disk[0]: Creating...
yandex_dns_zone.zone1: Creating...
yandex_alb_http_router.alb-router: Creation complete after 1s [id=ds77jv4kg6fnipvu1l5d]
yandex_dns_zone.zone1: Creation complete after 0s [id=dns2lb3gon4cimgbfkro]
yandex_dns_recordset.rs-2: Creating...
yandex_dns_recordset.rs-1: Creating...
module.network-create.yandex_vpc_gateway.nat_gateway: Creation complete after 1s [id=enpkq16mj4r50r596lgd]
yandex_dns_recordset.rs-1: Creation complete after 1s [id=dns2lb3gon4cimgbfkro/dip-akopalev.ru./A]
yandex_dns_recordset.rs-2: Creation complete after 1s [id=dns2lb3gon4cimgbfkro/www/CNAME]
module.network-create.yandex_vpc_network.network-1: Creation complete after 2s [id=enpj1evt5j0npctug9ro]
module.network-create.yandex_vpc_route_table.rt: Creating...
module.network-create.yandex_vpc_subnet.subnet-bast: Creating...
module.sg-create.yandex_vpc_security_group.sec-bast-sg: Creating...
module.network-create.yandex_vpc_subnet.subnet-bast: Creation complete after 1s [id=e2lv1cp0c39c09bnlnj5]
module.sg-create.yandex_vpc_security_group.sec-bast-sg: Creation complete after 2s [id=enpr0jg0cd0hncf6u97v]
module.bast-host.yandex_compute_instance.vm: Creating...
module.sg-create.yandex_vpc_security_group.external-sg: Creating...
module.sg-create.yandex_vpc_security_group.proxysql-sg: Creating...
module.network-create.yandex_vpc_route_table.rt: Creation complete after 2s [id=enpu5kqla1s38pl5lsq0]
module.network-create.yandex_vpc_subnet.subnet-3: Creating...
module.network-create.yandex_vpc_subnet.subnet-1: Creating...
module.network-create.yandex_vpc_subnet.subnet-2: Creating...
module.sg-create.yandex_vpc_security_group.db-sg: Creating...
module.network-create.yandex_vpc_subnet.subnet-1: Creation complete after 1s [id=e9b4npmrl1s0ko3p18s4]
module.sg-create.yandex_vpc_security_group.external-sg: Creation complete after 2s [id=enp4d8pse4r4m45h8rnh]
module.network-create.yandex_vpc_subnet.subnet-3: Creation complete after 2s [id=b0c8sdn6fbkm7mrenarn]
module.network-create.yandex_vpc_subnet.subnet-2: Creation complete after 3s [id=e2l1r02facsc762ajg0m]
yandex_alb_target_group.alb-tg: Creating...
yandex_alb_load_balancer.alb-1: Creating...
yandex_compute_disk.iscsi-disk[0]: Creation complete after 7s [id=fhmkq91mtdr85vble1lg]
module.sg-create.yandex_vpc_security_group.internal-sg: Creating...
yandex_alb_target_group.alb-tg: Creation complete after 0s [id=ds74f63ligcspnl0haie]
yandex_alb_backend_group.alb-bg: Creating...
yandex_alb_backend_group.alb-bg: Creation complete after 1s [id=ds7jcjm51ruq07v7c39q]
yandex_alb_virtual_host.alb-host: Creating...
module.sg-create.yandex_vpc_security_group.db-sg: Creation complete after 4s [id=enpl6ejst43ruaqhidl8]
module.db-servers["db-srv1"].yandex_compute_instance.vm: Creating...
module.db-servers["db-srv2"].yandex_compute_instance.vm: Creating...
module.db-servers["db-srv3"].yandex_compute_instance.vm: Creating...
yandex_alb_virtual_host.alb-host: Creation complete after 2s [id=ds77jv4kg6fnipvu1l5d/alb-host]
module.sg-create.yandex_vpc_security_group.internal-sg: Creation complete after 6s [id=enpmaqg0e1avss07iuet]
module.iscsi-srv.yandex_compute_instance.vm: Creating...
module.backends["backend1"].yandex_compute_instance.vm: Creating...
module.backends["backend3"].yandex_compute_instance.vm: Creating...
module.backends["backend2"].yandex_compute_instance.vm: Creating...
module.bast-host.yandex_compute_instance.vm: Still creating... [10s elapsed]
module.sg-create.yandex_vpc_security_group.proxysql-sg: Still creating... [10s elapsed]
module.sg-create.yandex_vpc_security_group.proxysql-sg: Creation complete after 12s [id=enpfj8ts1f3o4o7dj15f]
module.proxysql-srv.yandex_compute_instance.vm: Creating...
yandex_alb_load_balancer.alb-1: Still creating... [10s elapsed]
module.db-servers["db-srv1"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.db-servers["db-srv2"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.db-servers["db-srv3"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.iscsi-srv.yandex_compute_instance.vm: Still creating... [10s elapsed]
module.backends["backend3"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.backends["backend1"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.backends["backend2"].yandex_compute_instance.vm: Still creating... [10s elapsed]
module.bast-host.yandex_compute_instance.vm: Still creating... [20s elapsed]
module.proxysql-srv.yandex_compute_instance.vm: Still creating... [10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [20s elapsed]
module.db-servers["db-srv1"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.db-servers["db-srv2"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.db-servers["db-srv3"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.iscsi-srv.yandex_compute_instance.vm: Still creating... [20s elapsed]
module.backends["backend3"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.backends["backend1"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.backends["backend2"].yandex_compute_instance.vm: Still creating... [20s elapsed]
module.bast-host.yandex_compute_instance.vm: Still creating... [30s elapsed]
module.proxysql-srv.yandex_compute_instance.vm: Still creating... [20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [30s elapsed]
module.db-servers["db-srv1"].yandex_compute_instance.vm: Still creating... [30s elapsed]
module.db-servers["db-srv2"].yandex_compute_instance.vm: Still creating... [30s elapsed]
module.db-servers["db-srv3"].yandex_compute_instance.vm: Still creating... [30s elapsed]
module.db-servers["db-srv3"].yandex_compute_instance.vm: Creation complete after 32s [id=ef3aruso3c4gs0e2g1v5]
module.backends["backend3"].yandex_compute_instance.vm: Creation complete after 28s [id=ef36u8vgi8ftdjlghg53]
module.bast-host.yandex_compute_instance.vm: Creation complete after 38s [id=epdjv72tj1sa01u6iruj]
module.db-servers["db-srv2"].yandex_compute_instance.vm: Creation complete after 33s [id=epd71he23v122mvn929u]
module.iscsi-srv.yandex_compute_instance.vm: Still creating... [30s elapsed]
module.backends["backend1"].yandex_compute_instance.vm: Still creating... [30s elapsed]
module.backends["backend2"].yandex_compute_instance.vm: Still creating... [30s elapsed]
module.db-servers["db-srv1"].yandex_compute_instance.vm: Creation complete after 35s [id=fhmi9uvn5o4drstl7aqh]
module.backends["backend1"].yandex_compute_instance.vm: Creation complete after 33s [id=fhmkp5vrgj11p4vt02pa]
module.proxysql-srv.yandex_compute_instance.vm: Still creating... [30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [40s elapsed]
module.proxysql-srv.yandex_compute_instance.vm: Creation complete after 32s [id=fhmbu3ld0fr721cpdtbu]
module.backends["backend2"].yandex_compute_instance.vm: Creation complete after 38s [id=epdajhjqjnrhb2tv6i3g]
module.iscsi-srv.yandex_compute_instance.vm: Still creating... [40s elapsed]
module.iscsi-srv.yandex_compute_instance.vm: Creation complete after 41s [id=fhmd5413li80aueofcfe]
yandex_alb_load_balancer.alb-1: Still creating... [50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m0s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m40s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [1m50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m0s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m40s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [2m50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m0s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m40s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [3m50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m0s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m40s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [4m50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m0s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m10s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m20s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m30s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m40s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [5m50s elapsed]
yandex_alb_load_balancer.alb-1: Still creating... [6m0s elapsed]
yandex_alb_load_balancer.alb-1: Creation complete after 6m4s [id=ds7acecb7103jvopsd29]

Apply complete! Resources: 30 added, 0 changed, 0 destroyed.
